tokenizer: 
    en: /path/to/GPT2-small
    # en: neulab/gpt2-finetuned-wikitext103
    zh: 123
pretrained_model: 
    en: /path/to/GPT2-small
    # en: neulab/gpt2-finetuned-wikitext103
    zh: 123

# train configuration
train:
    load_param: true
    total_step: 100000
    save_every: 10000
    grad_clip: 0.1
    seed: 0
    batch_size: 16
    max_len: 1024

# test configuration
test:
    seed: 0
    batch_size: 1
    max_len: 1024
    # for ppl test
    ppl_max_len: 200
